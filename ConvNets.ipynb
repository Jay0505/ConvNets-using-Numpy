{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validity_check(image, conv_filter, padding, stride):\n",
    "    \n",
    "    is_valid = True\n",
    "    image_shape = (np.shape(image[0]))\n",
    "    \n",
    "    filter_shape = (np.shape(conv_filter))\n",
    "    \n",
    "    \n",
    "    if len(image_shape) != len(filter_shape) - 1:\n",
    "        print('Dimensions of both image and filter should be equal')\n",
    "        is_valid = False\n",
    "    \n",
    "    if len(image_shape) == 3 or len(filter_shape) > 3:\n",
    "        if image_shape[-1] != filter_shape[-1]:\n",
    "            print('Number of channels in both image and filter should be equal')\n",
    "            is_valid = False\n",
    "        \n",
    "    if filter_shape[1] != filter_shape[2]:\n",
    "        print(\"Filter should be of a square matrix\")\n",
    "        is_valid = False\n",
    "        \n",
    "        \n",
    "\n",
    "    if filter_shape[1] % 2 == 0:\n",
    "        print('dimensions of filter should be of odd dimensions not even')\n",
    "        is_valid = False\n",
    "    \n",
    "\n",
    "    '''\n",
    "    convolution_result is the array we obtain after running the filter all over the image\n",
    "    '''    \n",
    "    if is_valid:\n",
    "        '''one or more filters but only one channel (also for image)'''\n",
    "        if len(image_shape) == 2 and len(filter_shape) == 3:\n",
    "            \n",
    "            convolution_result = np.zeros((\n",
    "                            np.int16(((image_shape[0] + 2 * padding - filter_shape[1]) / stride) + 1),\n",
    "                            np.int16(((image_shape[1] + 2 * padding - filter_shape[2]) / stride) + 1)))\n",
    "            \n",
    "        '''one or more filters with more than one channel (also for image)'''\n",
    "        if len(image_shape) == 3 and len(filter_shape) == 4:\n",
    "            convolution_result = np.zeros((\n",
    "                            np.int16(((image_shape[0] + 2 * padding - filter_shape[1]) / stride) + 1),\n",
    "                            np.int16(((image_shape[1] + 2 * padding - filter_shape[2]) / stride) + 1),\n",
    "                            image_shape[3]))\n",
    "    else:\n",
    "        convolution_result = -1\n",
    "    \n",
    "    return [is_valid, convolution_result]\n",
    "\n",
    "\n",
    "def Relu(x):\n",
    "    indices = np.where(x < 0)\n",
    "    x[indices] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dimensions represent height, width and no_of_channels in the input image and filters\n",
    "'''\n",
    "\n",
    "\n",
    "class Conv_pool:\n",
    "    '''\n",
    "    self.conv_result, self.pooling_result and self.relu_result  are of shape (m, n, o, p)\n",
    "    m - length of  conv_result (or pooling_result or relu_result)\n",
    "    n - number of different conv_results (produced by each image in the input) or pooling_results\n",
    "    0 - number of rows in each entry in  conv_result (or pooling_result)\n",
    "    p - number of columns in each entry in conv_result (or pooling_result)\n",
    "    '''\n",
    "    def __init__(self, image, conv_filter, padding, stride, pooling_filter_size, pooling_filter_stride):\n",
    "        self.curr_image         = image\n",
    "        self.conv_filter        = conv_filter\n",
    "        self.conv_filter_stride = stride\n",
    "        self.pool_filter_size   = pooling_filter_size\n",
    "        self.pool_filter_stride = pooling_filter_stride\n",
    "        self.padding            = padding\n",
    "        self.conv_result        = np.array([])\n",
    "        self.relu_result        = np.array([])\n",
    "        self.pooling_result     = np.array([])\n",
    "            \n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    def apply_convolution_every_image(self, image, convolution_result):\n",
    "        conv_filter  = self.conv_filter\n",
    "        stride       = self.conv_filter_stride\n",
    "        image_shape  = np.shape(image)\n",
    "        filter_shape = np.shape(conv_filter)\n",
    "        \n",
    "        no_of_filters         = filter_shape[0]\n",
    "        convoluted_image_list = []\n",
    "        \n",
    "        '''this loop runs for all the different filters in the conv_filter'''\n",
    "        for filter_num in range(0, no_of_filters):\n",
    "            convoluted_image = np.array([])\n",
    "            filter_ = conv_filter[filter_num, :]\n",
    "    \n",
    "            '''if image has more than one channel'''\n",
    "            if len(image_shape) == 3 and image_shape[-1] == filter_shape[-1]:\n",
    "\n",
    "                convoluted_image = self.convolve_the_image_by_filter(image[:, :, 0], filter_[:, :, 0], \n",
    "                                                                   convolution_result)\n",
    "                for channel in range(1, filter_shape[-1]):\n",
    "                    convoluted_image = convoluted_image + self.convolve_the_image_by_filter(image[:, :, channel], \n",
    "                                                                                             filter_[:, :, channel],\n",
    "                                                                                                 convolution_result)\n",
    "            '''if image has only one channel'''\n",
    "            if len(image_shape) == 2:\n",
    "                \n",
    "                convoluted_image = self.convolve_the_image_by_filter(image, filter_, convolution_result)\n",
    "            '''here, you used copy.copy() so as to refrain the convoluted_image_list getting over-written\n",
    "            by the new entries into the list'''\n",
    "            \n",
    "            convoluted_image_list.append(copy.copy(convoluted_image))\n",
    "            \n",
    "            \n",
    "        return convoluted_image_list\n",
    "            \n",
    "    \n",
    "    ######################################################\n",
    "    \n",
    "    def start_convolving_the_image(self, convolution_result):\n",
    "        \n",
    "        arr_of_convs = []\n",
    "        no_of_images = len(self.curr_image)\n",
    "        \n",
    "        '''apply convolution for all the images in the input'''\n",
    "        for image_num in range(0, no_of_images):\n",
    "            \n",
    "            curr_img = self.curr_image[image_num, :]\n",
    "            convoluted_image_list = self.apply_convolution_every_image(curr_img, convolution_result)\n",
    "            arr_of_convs.append(convoluted_image_list)\n",
    "            \n",
    "        self.conv_result = np.array(arr_of_convs)\n",
    "\n",
    "     \n",
    "    \n",
    "    #########################################################\n",
    "    '''\n",
    "    when supplied with both image and filter, perform either convolution or pooling\n",
    "    '''\n",
    "    def apply_conv_or_pooling_on_data(self, image, filter_size, result, s\n",
    "                                                tride_length, is_pooling, conv_filter = None):\n",
    "        \n",
    "        row_count = 0\n",
    "        column_count = 0\n",
    "\n",
    "        image_shape = np.shape(image)[1]\n",
    "        print(np.shape(image))\n",
    "        for row in range(0, image_shape, stride_length):\n",
    "            if row + filter_size <= image_shape:\n",
    "                column_count = 0\n",
    "                for column in range(0, image_shape, stride_length):\n",
    "\n",
    "                    if column + filter_size <= image_shape:\n",
    "                        \n",
    "                        '''present active region is the region in the image which will be multiplied\n",
    "                            by the filter'''\n",
    "                        if not is_pooling:\n",
    "                            \n",
    "                            present_active_region           = image[row : (row + filter_size), \n",
    "                                                                          column : (column + filter_size)]\n",
    "                            image_area_product_filter       = present_active_region * conv_filter\n",
    "                            sum_of_all_elements             = np.sum(image_area_product_filter)\n",
    "                            result[row_count, column_count] = sum_of_all_elements\n",
    "                        \n",
    "                        if is_pooling:\n",
    "                            \n",
    "                            active_region       = image[row : (row + filter_size), \n",
    "                                                                    column : (column + filter_size)] \n",
    "                            result[row_count, column_count] = np.max(active_region)\n",
    "\n",
    "                            \n",
    "                        column_count = column_count + 1\n",
    "            row_count = row_count + 1\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "           \n",
    "    ######################################################\n",
    "    \n",
    "    def convolve_the_image_by_filter(self, image, conv_filter, convolution_result):\n",
    "        \n",
    "        stride      = self.conv_filter_stride\n",
    "        image_shape = np.shape(image)    \n",
    "        filter_size = np.shape(conv_filter)[0]\n",
    "        \n",
    "        result = self.apply_conv_or_pooling_on_data(image, filter_size, \n",
    "                                                    convolution_result, stride, False, conv_filter)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    \n",
    "    def relu_on_convolution_result(self):\n",
    "        convolution_result_ = self.conv_result\n",
    "        \n",
    "        arr_of_relus                 = []\n",
    "        conv_result_shape            = np.shape(convolution_result_)\n",
    "        no_of_images_convoluted      = conv_result_shape[0]\n",
    "        no_of_times_image_convoluted = conv_result_shape[1]\n",
    "        \n",
    "        \n",
    "        for image_num in range(0, no_of_images_convoluted):\n",
    "            \n",
    "            relu_result_list = []\n",
    "            curr_conv_result = convolution_result_[image_num, :]\n",
    "        \n",
    "        \n",
    "            for result_num in range(0, no_of_times_image_convoluted):\n",
    "                \n",
    "                current_convoluted_image = curr_conv_result[result_num, :]\n",
    "                indices = np.where(current_convoluted_image <= 0)\n",
    "                current_convoluted_image[indices] = 0\n",
    "                relu_result_list.append(current_convoluted_image)\n",
    "\n",
    "            arr_of_relus.append(relu_result_list)\n",
    "            \n",
    "        self.relu_result = np.array(arr_of_relus)\n",
    "\n",
    "        \n",
    "    ######################################################\n",
    "    \n",
    "    '''max pooling'''    \n",
    "    def pooling_on_relu_result(self):\n",
    "        relu_result = self.relu_result\n",
    "        pool_size   = self.pool_filter_size\n",
    "        stride      = self.pool_filter_stride\n",
    "        \n",
    "\n",
    "        relu_result_shape            = np.shape(relu_result)\n",
    "        no_of_images_relued          = relu_result_shape[0]\n",
    "        no_of_relu_results_per_image = np.shape(relu_result)[1]\n",
    "        relu_result_shape_each_image = np.shape(relu_result)[2]\n",
    "\n",
    "        arr_of_pools = []\n",
    "        \n",
    "        for image_num in range(0,  no_of_images_relued):\n",
    "    \n",
    "            curr_image_relu      = relu_result[image_num, :]\n",
    "            pooling_result_list  = []\n",
    "            for relu_result_num in range(0, no_of_relu_results_per_image):\n",
    "                curr_relu = curr_image_relu[relu_result_num, :]\n",
    "                pooling_result   = np.zeros((np.uint16(((relu_result_shape_each_image - pool_size) / stride) + 1),\n",
    "                                       np.uint16(((relu_result_shape_each_image - pool_size) / stride) + 1)))\n",
    "                curr_relu_result = curr_image_relu[relu_result_num, :]\n",
    "                result = self.apply_conv_or_pooling_on_data(image = curr_relu, filter_size = pool_size, \n",
    "                                                            result = pooling_result, stride_length = stride, \n",
    "                                                             is_pooling = True)\n",
    "                pooling_result_list.append(result)\n",
    "\n",
    "\n",
    "            arr_of_pools.append(pooling_result_list)\n",
    "        self.pooling_result = np.array(arr_of_pools)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    ######################################################\n",
    "    \n",
    "    def CNN(self):\n",
    "        \n",
    "        '''\n",
    "        validity_convolution_result[0] = Boolean_value\n",
    "        validity_convolution_result[1] = convolution_result\n",
    "        '''\n",
    "        \n",
    "        validity_convolution_result = validity_check(self.curr_image, self.conv_filter, \n",
    "                                                            self.padding, self.conv_filter_stride)\n",
    "        \n",
    "        if validity_convolution_result[0]:\n",
    "            convolution_result = validity_convolution_result[1]\n",
    "            self.start_convolving_the_image(convolution_result)\n",
    "            self.relu_on_convolution_result()\n",
    "            self.pooling_on_relu_result()\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(x):\n",
    "    indices = np.where(x <= 0)\n",
    "    x[indices] = 0\n",
    "    return x\n",
    "    \n",
    "def softmax(x):\n",
    "    x = x.reshape(1, -1)\n",
    "    \n",
    "    max_item = np.max(x)\n",
    "    print('max is ' + str(max_item))\n",
    "    exp_x = np.exp(x - max_item)\n",
    "    res = np.true_divide(exp_x, np.sum(exp_x))\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy(output, labels):\n",
    "    return -np.sum(labels * np.log(output))\n",
    "\n",
    "\n",
    "\n",
    "class fully_connected_layers_in_CNN:\n",
    "    def __init__(self, conv_pool_object, labels, no_of_neurons):\n",
    "        self.conv_pool_result = conv_pool_object\n",
    "        self.pooling_result = conv_pool_object.pooling_result\n",
    "        self.labels = labels\n",
    "        self.pooling_result_shape = np.shape(self.pooling_result)\n",
    "        self.neurons = no_of_neurons\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def feed_forward_in_fc_layers(self):\n",
    "        pooling_result = self.pooling_result\n",
    "        no_of_neurons = self.neurons\n",
    "        no_of_pool_results, no_of_rows, no_of_columns = self.pooling_result_shape\n",
    "        self.fc_layer_1 = pooling_result.reshape(((no_of_pool_results * no_of_rows * no_of_columns), 1))\n",
    "        \n",
    "        no_of_rows_in_fc_layer_1 = self.fc_layer_1.shape[0]\n",
    "        self.weights_1 = np.random.random((no_of_neurons[0], no_of_rows_in_fc_layer_1))\n",
    "        self.bias_1 = np.zeros(shape = (no_of_neurons[0], 1))\n",
    "        \n",
    "        self.z_1 = np.add(np.dot(self.weights_1, self.fc_layer_1), self.bias_1)\n",
    "        print(np.shape(self.z_1))\n",
    "        self.fc_layer_2 = Relu(self.z_1)\n",
    "        \n",
    "        no_of_rows_in_fc_layer_2 = self.fc_layer_2.shape[0]\n",
    "        self.weights_2 = np.random.random((no_of_neurons[1], no_of_rows_in_fc_layer_2))\n",
    "        self.bias_2 = np.zeros(shape = (no_of_neurons[1], 1))\n",
    "        \n",
    "        self.z_2 = np.add(np.dot(self.weights_2, self.fc_layer_2), self.bias_2)\n",
    "        \n",
    "        self.predicted_probs = softmax(self.z_2)\n",
    "        print('pr ' + str(self.predicted_probs))\n",
    "        self.loss = cross_entropy(self.predicted_probs, self.labels)\n",
    "        self.diff_between_pred_and_actual_labels = self.predicted_probs - self.labels\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class back_propagation_in_CNN:\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def __init__(self, fc_layer_object):\n",
    "        self.fc_layer_object = fc_layer_object\n",
    "        self.conv_pool_result = fc_layer_object.conv_pool_result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def get_the_index(self, conv_sub_array):\n",
    "        position_of_max_element = np.nanargmax(conv_sub_array)\n",
    "        index_of_max_element = np.unravel_index(position_of_max_element, conv_sub_array.shape)\n",
    "        return index_of_max_element\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def backpropagation_wrt_pooling_result(self, derivative_pool_result):\n",
    "        \n",
    "        \n",
    "        conv_result = self.conv_pool_result.conv_result\n",
    "        pool_filter_size = self.conv_pool_result.pool_filter_size\n",
    "        pool_filter_stride = self.conv_pool_result.pool_filter_stride\n",
    "        \n",
    "        no_of_convolved_images, no_of_rows, no_of_columns = np.shape(conv_result)\n",
    "        _, pool_row_length, _ = np.shape(derivative_pool_result)\n",
    "        output = np.zeros(shape = np.shape(conv_result))\n",
    "    \n",
    "        row_count = 0\n",
    "        column_count = 0\n",
    "        for image_num in range(0, no_of_convolved_images):\n",
    "            current_image = conv_result[image_num, :]\n",
    "            row_count = 0\n",
    "            for row in range(0, no_of_rows, pool_filter_stride):\n",
    "                if row + pool_filter_size <= no_of_rows:\n",
    "                    column_count = 0\n",
    "                    for column in range(0, no_of_columns, pool_filter_stride):\n",
    "                        if column + pool_filter_size <= no_of_columns:\n",
    "                            image_sub_array = current_image[row : row + pool_filter_size, column : column + pool_filter_size]\n",
    "                            '''index of the max element in the original conv_image (in the area covered by pool filter)'''\n",
    "                            (i, j) = self.get_the_index(image_sub_array)\n",
    "                            if row_count <= pool_row_length:\n",
    "                                output[image_num, row + i, column + j] = derivative_pool_result[image_num, \n",
    "                                                                                            row_count, column_count]\n",
    "                            column_count = column_count + 1\n",
    "                \n",
    "                row_count = row_count + 1\n",
    "            \n",
    "        return output\n",
    "                              \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def backpropagation_wrt_convolution_layer(self, der_conv_prev):\n",
    "                \n",
    "        conv_curr_image = self.conv_pool_result.curr_image\n",
    "        conv_image_curr_list = [conv_curr_image]\n",
    "        conv_image_curr = np.array(conv_image_curr_list)\n",
    "        conv_filter = self.conv_pool_result.conv_filter\n",
    "        filter_stride = self.conv_pool_result.conv_filter_stride\n",
    "        no_of_filters, flt_size, no_of_columns_f = np.shape(conv_filter)\n",
    "        \n",
    "        len_of_shape_of_curr_image = len(np.shape(conv_image_curr))\n",
    "        \n",
    "        _,no_of_rows, no_of_columns = np.shape(conv_image_curr)\n",
    "\n",
    "        \n",
    "        der_wrt_filter = np.zeros(np.shape(conv_filter))\n",
    "        der_wrt_bias = np.zeros((no_of_filters, 1))\n",
    "        dconv_result = np.zeros(np.shape(conv_image_curr))\n",
    "        \n",
    "        r_count = 0\n",
    "        c_count = 0\n",
    "        \n",
    "        for filter_num in range(0, no_of_filters, filter_stride):\n",
    "            r_count = 0\n",
    "            for r in range(0, no_of_rows):\n",
    "                if r + flt_size <= no_of_rows:\n",
    "                    c_count = 0\n",
    "                    for c in range(0, no_of_columns, filter_stride):\n",
    "                        if c + flt_size <= no_of_columns:\n",
    "                            '''update the filter'''\n",
    "                            \n",
    "                            der_wrt_filter += der_conv_prev[filter_num, r_count, c_count] * conv_image_curr[:, \n",
    "                                                                                                         r : r + flt_size,\n",
    "                                                                                                         c : c + flt_size]\n",
    "                            \n",
    "                            dconv_result[:, r : r + flt_size, c : c + flt_size] += der_conv_prev[filter_num, r_count, c_count] * conv_filter[filter_num, :]\n",
    "                            \n",
    "                            c_count = c_count + 1\n",
    "                r_count = r_count + 1\n",
    "        \n",
    "            der_wrt_bias[filter_num] = np.sum(der_conv_prev[filter_num])\n",
    "        \n",
    "        \n",
    "        return dconv_result, der_wrt_filter, der_wrt_bias\n",
    "                                \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    def start_back_propagation(self):\n",
    "        fc_layer_object = self.fc_layer_object\n",
    "        fc_layer_1 = fc_layer_object.fc_layer_1\n",
    "        self.derivative_wrt_weights_2  = np.dot(fc_layer_object.diff_between_pred_and_actual_labels.T, \n",
    "                                                       fc_layer_object.z_1.T)\n",
    "        \n",
    "        temporary_result = np.dot(fc_layer_object.weights_2.T,  \n",
    "                                                    fc_layer_object.diff_between_pred_and_actual_labels.T)\n",
    "        \n",
    "        print(fc_layer_object.fc_layer_1)\n",
    "        self.derivative_wrt_weights_1 = np.dot(temporary_result,\n",
    "                                                  fc_layer_object.fc_layer_1.T)\n",
    "        \n",
    "        self.derivative_wrt_bias_2 = np.sum(fc_layer_object.diff_between_pred_and_actual_labels, \n",
    "                                                axis = 1).T\n",
    "        self.derivative_wrt_bias_1 = np.sum(temporary_result, axis = 1).reshape(fc_layer_object.bias_1.shape)\n",
    "        \n",
    "        \n",
    "        der_fc_layer_1 = np.dot(fc_layer_object.weights_1.T, temporary_result)\n",
    "        reshaped_dfc_layer_1 = der_fc_layer_1.reshape(fc_layer_object.pooling_result_shape)\n",
    "        '''back propagating through max-pooling layer (updating only those neurons with highest value)'''\n",
    "        der_pool_layer = self.backpropagation_wrt_pooling_result(reshaped_dfc_layer_1)\n",
    "        \n",
    "        '''back propagating through Relu layer'''\n",
    "        der_pool_layer[fc_layer_object.conv_pool_result.conv_result <= 0] = 0\n",
    "        \n",
    "        der_image, der_filter, der_conv_bias = self.backpropagation_wrt_convolution_layer(der_pool_layer)\n",
    "        \n",
    "        self.return_result = [der_filter, der_conv_bias]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8)\n",
      "(8, 8)\n",
      "(8, 8)\n",
      "(8, 8)\n",
      "(6, 6)\n",
      "(6, 6)\n",
      "(6, 6)\n",
      "(6, 6)\n"
     ]
    }
   ],
   "source": [
    "l1_filter = np.zeros((2,3,3))\n",
    "\n",
    "l1_filter[0, :, :] = np.array([[[-1, 0, 1],   \n",
    "                                  [-1, 0, 1],   \n",
    "                                   [-1, 0, 1]]])  \n",
    "l1_filter[1, :, :] = np.array([[[1,   1,  1],   \n",
    "                                   [0,   0,  0],   \n",
    "                                    [-1, -1, -1]]]) \n",
    "\n",
    "image_1 = np.random.uniform(low = 0, high = 1, size = (8, 8))\n",
    "image_2 = np.random.uniform(low = 0, high = 1, size = (8, 8))\n",
    "image = np.array([image_1, image_2])\n",
    "\n",
    "cnn = Conv_pool(image, l1_filter, 0, 1, 2, 2)\n",
    "cnn.CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.75122844, 0.19901213],\n",
       "        [1.99486622, 0.20382838, 0.43536085],\n",
       "        [1.90555471, 0.30021249, 1.21539214]],\n",
       "\n",
       "       [[0.90325235, 1.59180576, 1.11952759],\n",
       "        [0.80867883, 0.08104513, 0.        ],\n",
       "        [0.77199353, 0.39044037, 0.14608003]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cnn.pooling_result)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low = 0, high = 20, size = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0550647  0.83951965 0.18522795 0.08386781 0.35454334 0.59298627\n",
      "  0.78457192 0.03404839]\n",
      " [0.95565334 0.39545117 0.03907397 0.92021912 0.70602216 0.87389992\n",
      "  0.11821481 0.03111724]\n",
      " [0.00812017 0.56634476 0.58355534 0.38592402 0.4160145  0.87923553\n",
      "  0.384459   0.68813559]\n",
      " [0.73067203 0.70937883 0.06138757 0.47535108 0.53887899 0.69277726\n",
      "  0.15300017 0.48714134]\n",
      " [0.11198032 0.49223351 0.35898734 0.05932251 0.34018652 0.95632993\n",
      "  0.6948106  0.99277468]\n",
      " [0.40080755 0.71693071 0.16046499 0.05338222 0.4467253  0.0430063\n",
      "  0.4192921  0.01485212]\n",
      " [0.00767441 0.46227389 0.7079541  0.15649137 0.51665108 0.12283493\n",
      "  0.26025794 0.11353354]\n",
      " [0.00569858 0.38398884 0.84689602 0.98643414 0.64524681 0.84656047\n",
      "  0.34218255 0.20739032]]\n",
      "[[0.24237134 0.72422269 0.47493803 0.09080515 0.28915886 0.89303331\n",
      "  0.43415994 0.54804265]\n",
      " [0.12902376 0.35094135 0.02964232 0.47700454 0.70893824 0.02000631\n",
      "  0.48480454 0.59718791]\n",
      " [0.39099842 0.853302   0.10030094 0.31821206 0.53865502 0.94441426\n",
      "  0.01362039 0.11543075]\n",
      " [0.8251528  0.22851753 0.5021731  0.94878491 0.86878104 0.02890033\n",
      "  0.52794673 0.26979314]\n",
      " [0.54210121 0.57980608 0.76543956 0.89192644 0.55946591 0.03846395\n",
      "  0.29796958 0.34027301]\n",
      " [0.5998699  0.54421121 0.30307976 0.77089653 0.09003862 0.87724907\n",
      "  0.32155415 0.22693936]\n",
      " [0.81306304 0.1821952  0.56822939 0.50702783 0.56294991 0.6946631\n",
      "  0.65548218 0.16915267]\n",
      " [0.84057335 0.58705032 0.4743248  0.3306926  0.45772021 0.47918261\n",
      "  0.47833735 0.10416767]]\n"
     ]
    }
   ],
   "source": [
    "for r in range(0, len(image)):\n",
    "    print(image[r, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
